{
  "hash": "e0a490a4f6c940ee7701b8c2389552d7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Training GNN on boids (16 types)\nauthor: CÃ©dric Allier, Michael Innerberger, Stephan Saalfeld\ncategories:\n  - Particles\nexecute:\n  echo: false\nimage: \"create_fig_boids_files/figure-html/cell-4-output-1.png\"\n---\n\n\n\n\nThis script generates figures shown in Supplementary Figures 4, 11 and 12.\nA GNN learns the motion rules of boids (https://en.wikipedia.org/wiki/Boids).\nThe simulation used to train the GNN consists of 1792 particles of 16 different types.\nThe boids interact with each other according to 16 different laws.\n\n\n\nFirst, we load the configuration file and set the device.\n\n::: {#e848f95e .cell execution_count=2}\n``` {.python .cell-code}\nconfig_file = 'boids_16_256'\nfigure_id = 'supp4'\nconfig = ParticleGraphConfig.from_yaml(f'./config/{config_file}.yaml')\ndevice = set_device(\"auto\")\n```\n:::\n\n\nThe following model is used to simulate the boids system with PyTorch Geometric.\n\n::: {#53aada70 .cell execution_count=3}\n``` {.python .cell-code}\nclass BoidsModel(pyg.nn.MessagePassing):\n    \"\"\"Interaction Network as proposed in this paper:\n    https://proceedings.neurips.cc/paper/2016/hash/3147da8ab4a0437c15ef51a5cc7f2dc4-Abstract.html\"\"\"\n\n    \"\"\"\n    Compute the acceleration of Boids as a function of their relative positions and relative positions.\n    The interaction function is defined by three parameters p = (p1, p2, p3)\n\n    Inputs\n    ----------\n    data : a torch_geometric.data object\n\n    Returns\n    -------\n    pred : float\n        the acceleration of the Boids (dimension 2)\n    \"\"\"\n\n    def __init__(self, aggr_type=[], p=[], bc_dpos=[], dimension=2):\n        super(BoidsModel, self).__init__(aggr=aggr_type)  # \"mean\" aggregation.\n\n        self.p = p\n        self.bc_dpos = bc_dpos\n        self.dimension = dimension\n\n        self.a1 = 0.5E-5\n        self.a2 = 5E-4\n        self.a3 = 1E-8\n        self.a4 = 0.5E-5\n        self.a5 = 1E-8\n\n    def forward(self, data=[], has_field=False):\n        x, edge_index = data.x, data.edge_index\n\n        if has_field:\n            field = x[:,6:7]\n        else:\n            field = torch.ones_like(x[:,0:1])\n\n        edge_index, _ = pyg_utils.remove_self_loops(edge_index)\n        particle_type = to_numpy(x[:, 1 + 2*self.dimension])\n        parameters = self.p[particle_type, :]\n        d_pos = x[:, self.dimension+1:1 + 2*self.dimension].clone().detach()\n        dd_pos = self.propagate(edge_index, pos=x[:, 1:self.dimension+1], parameters=parameters, d_pos=d_pos, field=field)\n\n        return dd_pos\n\n    def message(self, pos_i, pos_j, parameters_i, d_pos_i, d_pos_j, field_j):\n        distance_squared = torch.sum(self.bc_dpos(pos_j - pos_i) ** 2, axis=1)  # distance squared\n\n        cohesion = parameters_i[:,0,None] * self.a1 * self.bc_dpos(pos_j - pos_i)\n        alignment = parameters_i[:,1,None] * self.a2 * self.bc_dpos(d_pos_j - d_pos_i)\n        separation = - parameters_i[:,2,None] * self.a3 * self.bc_dpos(pos_j - pos_i) / distance_squared[:, None]\n\n        return (separation + alignment + cohesion) * field_j\n\n\ndef bc_pos(x):\n    return torch.remainder(x, 1.0)\n\n\ndef bc_dpos(x):\n    return torch.remainder(x - 0.5, 1.0) - 0.5\n```\n:::\n\n\nThe training data is generated with the above Pytorch Geometric model\n\nVizualizations of the boids motion can be found in \"decomp-gnn/paper_experiments/graphs_data/graphs_boids_16_256/Fig/\"\n\nIf the simulation is too large, you can decrease n_particles (multiple of 16) in \"boids_16_256.yaml\"\n\n::: {#874ef3c4 .cell execution_count=4}\n``` {.python .cell-code}\np = torch.squeeze(torch.tensor(config.simulation.params))\nmodel = BoidsModel(aggr_type=config.graph_model.aggr_type, p=torch.squeeze(p), bc_dpos=bc_dpos, dimension=config.simulation.dimension)\n\ngenerate_kwargs = dict(device=device, visualize=True, run_vizualized=0, style='color', alpha=1, erase=True, save=True, step=100)\ntrain_kwargs = dict(device=device, erase=True)\ntest_kwargs = dict(device=device, visualize=True, style='color', verbose=False, best_model='20', run=0, step=50, save_velocity=True)\n\n# data_generate_particles(config, model, bc_pos, bc_dpos, **generate_kwargs)\n```\n:::\n\n\n::: {#15fc8620 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![Initial configuration of the simulation. There are 1792 boids. The colors indicate different types.](create_fig_boids_files/figure-html/cell-6-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {#d921c0d0 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![Frame 7500 out of 8000](create_fig_boids_files/figure-html/cell-7-output-1.png){width=470 height=470}\n:::\n:::\n\n\nThe GNN model (see src/ParticleGraph/models/Interaction_Particle.py) is trained and tested.\n\nSince we ship the trained model with the repository, this step can be skipped if desired.\n\n::: {#40b6cc43 .cell execution_count=7}\n``` {.python .cell-code}\nif not os.path.exists(f'log/try_{config_file}'):\n    data_train(config, config_file, **train_kwargs)\n```\n:::\n\n\nDuring training the plot of the embedding are saved in\n\"paper_experiments/log/try_boids_16_256/tmp_training/embedding\"\nThe plot of the pairwise interactions are saved in\n\"paper_experiments/log/try_boids_16_256/tmp_training/function\"\n\nThe model that has been trained in the previous step is used to generate the rollouts.\n\n::: {#f386ddd7 .cell execution_count=8}\n``` {.python .cell-code}\ndata_test(config, config_file, **test_kwargs)\n```\n:::\n\n\nFinally, we generate figures from the post-analysis of the GNN.\nThe results of the GNN post-analysis are saved into 'decomp-gnn/paper_experiments/log/try_boids_16_256/results'.\n\n::: {#b9e67700 .cell execution_count=9}\n``` {.python .cell-code}\nconfig_list, epoch_list = get_figures(figure_id, device=device)\n```\n:::\n\n\n::: {#a9527d83 .cell execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![Learned latent vectors (x4800)](create_fig_boids_files/figure-html/cell-11-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {#34d819cb .cell execution_count=11}\n\n::: {.cell-output .cell-output-display}\n![Learned interaction functions (x16)](create_fig_boids_files/figure-html/cell-12-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {#cc8a6fe4 .cell execution_count=12}\n\n::: {.cell-output .cell-output-display}\n![Learned cohesion parameters](create_fig_boids_files/figure-html/cell-13-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {#25909533 .cell execution_count=13}\n\n::: {.cell-output .cell-output-display}\n![Learned alignment parameters](create_fig_boids_files/figure-html/cell-14-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {#4f196879 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display}\n![Learned separation parameters](create_fig_boids_files/figure-html/cell-15-output-1.png){width=470 height=470}\n:::\n:::\n\n\n::: {#36b80b23 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display}\n![GNN rollout inference at frame 7950](create_fig_boids_files/figure-html/cell-16-output-1.png){width=470 height=470}\n:::\n:::\n\n\nAll frames can be found in \"decomp-gnn/paper_experiments/log/try_boids_16_256/tmp_recons/\"\n\n\n",
    "supporting": [
      "create_fig_boids_files"
    ],
    "filters": [],
    "includes": {}
  }
}